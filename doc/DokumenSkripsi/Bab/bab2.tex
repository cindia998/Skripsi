%versi 2 (8-10-2016)
\chapter{Landasan Teori}
\label{chap:teori}

Pada bab ini akan dijelaskan mengenai dasar-dasar teori yang akan digunakan dalam penyusunan Skripsi ini. Karena penyusunan skripsi ini menggunakan Apache Spark maka teori yang akan dibahas pertama kali pada bab ini adalah penjelasan mengenai Spark dan cara kerjanya. Teori berikutnya adalah penjelasan mengenai RDD dan cara kerjanya, dan juga fungsi-fungsi RDD yang ada. Setelah itu, akan dijelaskan juga mengenai klasifikasi, apa saja klasifikasi yang ada pada Spark MLlib begitu pula dengan penjelasan source code dari algoritma tertentu yang akan dijadikan algoritma secara inkremental. 

\section{Apache Spark[]}
\label{sec:spark} 
Apache spark adalah komputasi cluster cepat yang dirancang untuk komputasi cepat[]. Maksud dari komputasi cluster adalah suatu komputasi dilakukan oleh lebih dari satu komputer. Dengan kata lain, komputasi dilakukan bersama-sama secara paralel dalam lebih dari satu komputer. Komputasi cepat disini dikarenakan Apache spark dibangun di atas Hadoop MapReduce dan Spark meningkatkan model MapReduce yang sudah ada agar dapat menggunakan lebih banyak jenis perhitungan secara efisien. Sama seperti Hadoop, Spark dirancang untuk memproses Big Data. Spark dapat memproses berbagai data yang besar dan berat seperti aplikasi batch, algoritma iteratif, query yang interaktif dan streaming. Yang membedakan Spark dan Hadoop adalah fitur utama Spark dimana Spark menjalankan komputasi cluster di dalam memori sehingga dapat meningkatkan kecepatan pemrosesan aplikasi.

Apache Spark digunakan karena banyaknya kelebihan-kelebihan yang dimilikinya yakni:
\begin{enumerate}
\item Speed\\
Spark membantu menjalankan aplikasi di \textit{cluster Hadoop} hingga 100 kali lebih cepat karena komputasi dalam memori, dan 10 kali lebih cepat saat dijalankan pada disk. Hal tersebut dapat terjadi dengan cara mengurangi jumlah operasi baca tulis ke disk. Hal tersebut dimungkinkan dengan menyimpan pemrosesan data di dalam memori.
\item Powerful Caching\\
Spark mendukung layer pemrograman sederhana sehingga memberikan kemampuan cache yang kuat dan juga kemampuan disk yang persistence. 
\item Deployment\\
Spark dapat dipasang di berbagai platform misal Mesos, Hadoop via Yarn atau cluster manager milik spark sendiri.
\item Real-time\\
Spark menyediakan komputasi \textit{Real Time} dan \textit{low latency} dikarenakan Spark menjalankan komputasi di dalam memori.
\item Polygot\\
Spark dapat diimplementasikan dalam banyak bahasa yakni bahasa Java, Scala, dan Python.
\item Scalable
Sama seperti Hadoop, Spark juga mendukung menambahkan node-node baru ke dalam cluster yang ada. Jika semakin banyak node yang ditambahkan maka pemrosesan data akan semakin cepat. Pemrosesan data juga akan jadi lebih murah karena banyak komputer dengan spesifikasi standar lebih murah dari pada dengan membeli komputer dengan spesifikasi tinggi.
\end{enumerate}

Spark memiliki banyak komponen-komponen yang mendukung pemrosesan \textit{big data}. Satu komponen inti dari Spark yang harus ada dalam setiap program Spark adalah Spark Core. Komponen-komponen yang lain hanya komponen pendukung Spark yang sifatnya opsional(dapat dipakai maupun tidak), akan tetapi tentu saja juga mempermudah proses komputasi big data. Berikut adalah penjelasan singkat mengenai komponen-komponen yang ada pada Spark.
\begin{enumerate}
\item Apache Spark Core\\
Spark Core adalah komponen yang mendasari semua platform spark dengan semua fungsi-fungsinya. Dengan kata lain, Spark Core adalah komponen yang harus ada jika ingin menggunakan fungsi-fungsi yang ada pada Spark. Spark Core menyediakan komputasi dalam memory dan mereferensikan dataset-dataset di dalam sistem penyimpanan eksternal.
\item Spark SQL \\
Spark SQL adalah komponen diatas SparkCore yang menyediakan abstraksi data yang disebut SchemaRDD. SchemaRDD menyediakan bantuan untuk kueri-kueri data terstruktur dan data semi-terstruktur.
\item Spark Streaming\\
Spark Streaming memanfaatkan kemampuan penjadwalan cepat dari Spark Core untuk melakukan analisis streaming. Analisis streaming mengambil data dalam batch-batch kecil dan menjalankan transformasi RDD (Resilient Distributed Datasets) pada data di dalam batch-batch kecil tersebut.
\item MLlib(Machine Learning Library)\\
MLlib adalah framework machine learning terdistribusi diatas Spark dikarenakan arsitektur Spark yang berbasis memori terdistribusi. Spark MLLib menyediakan algoritma-algoritma yang dapat digunakan untuk analisis data seperti regresi, klasifikasi, dan lain-lain.
\item GraphX\\
GraphX adalah framework untuk pemrosesan grafik terdistribusi di atas spark. GraphX menyediakan API untuk membuat perhitungan grafik yang dapat memodelkan grafik yang ditentukan user.
\item SparkR\\
SparkR adalah package milik R yang menyediakan implementasi data frame terdistribusi. SparkR juga mendukung operasi seperti seleksi, filter, agregasi untuk dataset dataset yang ukurannya besar.

\end{enumerate}

\subsection{Resilient Distributed Dataset}
RDD adalah bagian inti dari semua aplikasi Spark. Dataset-dataset yang ingin diproses menggunakan Spark akan dibaca melalui Spark Context sebagai objek RDD. RDD adalah dataset yang tersebar ke semua node di dalam sebuah cluster dan data-data tersebut tahan atau toleran terhadap kesalahan dan kerusakan serta dapat mengembalikan data yang gagal diproses. Secara umum, RDD bersifat immutable. Immutable artinya objek tersebut tidak dapat diubah setelah dibuat, akan tetapi objek tersebut dapat ditransformasi serta melakukan aksi. 

Ada 2 cara membuat RDD, cara pertama adalah dengan memparalelkan koleksi data yang ada di dalam driver program dan cara yang kedua adalah dengan mereferensi dataset di luar storage system seperti shared file system, HDFS, HBase, dan lain-lain. Cara kerja RDD adalah dataset di dalam RDD dibagi menjadi beberapa bagian berdasarkan suatu key. Bagian-bagian data tersebut kemudian akan dibagikan ke seluruh node-node pekerja. Karena itu, RDD memungkinkan untuk melakukan perhitungan fungsi terhadap dataset dengan sangat cepat dengan memanfaatkan beberapa node pekerja. Satu node pekerja dapat menyimpan lebih dari satu bagian data. Selain itu, bagian data yang sama dapat direplikasi di beberapa node pekerja. Jadi, jika satu node pelaksana gagal atau mengalami kesalahan (error) maka yang node yang lain masih bisa memproses data. Oleh karena itulah, RDD disebut dan bersifat sangat \textit{resilient} dimana RDD kebal terhadap kegagalan atau kesalahan dan dapat pulih dengan cepat dari kesalahan. 

Setiap dataset di dalam RDD dibagi menjadi partisi yang logis, yang artinya dapat di komputasi atau dijalankan di node yang berbeda dalam sebuah cluster. Karena sifatnya ini, kita dapat melakukan transformasi atau aksi ke seluruh data secara paralel. Distribusinya dilakukan secara otomatis oleh Spark. 
RDD bisa melakukan 2 operasi yakni transformasi dan aksi. Transformasi merupakan operasi RDD yang digunakan untuk membuat RDD baru dari RDD yang sudah ada. Contohnya adalah operasi map, filter, dan masih banyak lagi. Operasi yang kedua adalah aksi. Aksi dipakai dalam RDD untuk memberitahukan Spark untuk menggunakan komputasi dan memberikan hasilnya kembali ke driver. Contoh dari operasi ini adalah menuliskan RDD hasil transformasi ke dalam file text(.txt) ke driver komputer atau mengembalikan hanya sebuah value.

\subsubsection{Transformasi RDD}
RDD dapat melakukan berbagai macam operasi transformasi. Semua operasi transformasi yang dilakukan oleh objek RDD akan menghasilkan RDD baru. Berikut adalah operasi-operasi tranformasi yang dapat dilakukan oleh objek RDD.\\

\begin{tabular}{|p{0.05\textwidth}|p{0.3\textwidth}|p{0.55\textwidth}|}
\hline 
No. & Transformasi & Arti \\ 
\hline 
1 & map(func) & Mengembalikan RDD baru, terbentuk dari memasukkan setiap elemen melalui fungsi func. \\ 
\hline 
2 & filter(func) & Mengembalikan dataset baru, terbentuk dari memilih elemen yang bernilai \textit{true} dari fungsi func.\\ 
\hline 
3 & flatMap(func) & Mirip dengan map, tapi setiap \textit{input item} dapat di map ke 0 atau lebih \textit{output item} (jadi fungsi func harus mengembalikan sebuah \textit{Sequence} daripada hanya sebuah \textit{item}.\\ 
\hline 
4 & mapPartitions(func) & Mirip dengan map, tapi berjalan secara terpisah di setiap partisi(blok) dari RDD, jadi fungsi func harus bertipe Iterator<T> => Iterator<U> saat berjalan dalam sebuah RDD tipe T.\\ 
\hline 
5 & mapPartitionsWithIndex(func) & Mirip dengan mapPartitions, akan tetapi juga menyediakan fungsi func dengan sebuah nilai integer representasi dari index dari partisi, jadi fungsi func harus bertipe (Int, Iterator<T>) => Iterator<U> saat berjalan di sebuah RDD dengan tipe T. \\ 
\hline 
6 & sample(withReplacement, fraction, seed) & Membuat sampel dari sebagian (\textit{fraction}) data, dengan atau tidak replacement, menggunakan \textit{random number generator seed} yang diberikan.\\ 
\hline 
7 & union(otherDataset) & Mengembalikan dataset baru yang berisi gabungan (union) elemen dari dataset dan dataset dari parameter.\\ 
\hline 
8 & intersection(otherDataset) & Mengembalikan RDD baru yang berisi irisan (intersection) antara elemen dari dataset dan dataset dari parameter.\\ 
\hline 
\end{tabular}


\begin{tabular}{|p{0.05\textwidth}|p{0.3\textwidth}|p{0.55\textwidth}|}
\hline
9 & distinct([numTasks]) & Mengembalikan dataset baru yang berisi elemen yang berbeda-beda dari dataset.\\ 
\hline 
10 & groupByKey([numTasks]) & Saat dipanggil pada dataset dari pasangan (K,V), mengembalikan dataset dari pasangan (K, Iterable<V>). \\ 
\hline
11 & reduceByKey(func,[numTasks]) & Saat dipanggil pada dataset dari pasangan (K,V), mengembalikan dataset dari pasangan (K,V) dimana values dari setiap key diagregasi menggunakan fungsi reduce func yang diberikan di parameter, dimana harus bertipe (V,V)=> V. Seperti pada groupByKey, jumlah dari tugas reduce dapat dikonfigurasi melalui parameter kedua yang opsional. \\ 
\hline
12 & aggregateByKey(zeroValue) (seqOp, combOp, [numTasks]) & Saat dipanggil pada dataset dari pasangan (K,V), mengembalikan dataset dari pasangan (K,U) dimana values dari setiap key diagregasi menggunakan fungsi penggabungan yang diberikan (combOP) dan sebuah value "nol" netral. Memungkinkan sebuah tipe value yang diagregasi yang berbeda dari tipe value input, sambil menghindari alokasi yang tidak perlu. Seperti pada groupByKey, jumlah dari tugas reduce dapat dikonfigurasi melalui parameter kedua yang opsional.\\ 
\hline
13 & sortByKey([ascending], [numTasks]) & Saat dipanggil pada dataset dari pasangan (K,V) dimana K mengimplementasikan Ordered, mengembalikan dataset dari pasangan (K,V) terurut berdasarkan keys secara menaik atau menurun, sesuai parameter Boolean ascending.\\ 
\hline
14 & join(otherDataset,[numTasks]) & Saat dipanggil pada dataset dari tipe (K,V) dan (K,W), mengembalikan dataset dari pasangan (K,(V,W)) dengan semua pasangan dari elemen tiap key. Outer joins didukung dengan leftOuterJoin, rightOuterJoin, dan fullOuterJoin.\\ 
\hline
15 & cogroup(otherDataset, [numTasks]) & Saat dipanggil pada dataset dari tipe (K,V) dan (K,W), mengembalikan sebuah dataset dari tuple (K,Iterable<V>,Iterable<W>)). Operasi ini juga disebut 'group With'. \\ 
\hline
16 & cartesian(otherDataset) & Saat dipanggil pada dataset dari tipe T dan U, mengembalikan sebuah dataset dari pasangan (T,U) (semua pasangan elemen)\\ 
\hline
17 & pipe(command, [envVars]) &  Pipe setiap partisi dari RDD melalui shell command, contohnya Perl atau bash script. Elemen RDD ditulis ke proses stdin dan baris output ke stdout dikembalikan sebagai RDD strings\\ 
\hline
18 & coalesce(numPartitions) & Mengurangi jumlah partisi di dalam RDD hingga numPartitions. Berguna untuk menjalankan operasi secara lebih efisien setelah memfilter dataset berukuran besar\\ 
\hline
19 & repartition(numPartitions) & Mengacak ulang data di dalam RDD secara acak untuk menciptakan antara lebih sedikit partisi dan menyeimbangkannya. Method ini selalu mengacak semua data di jaringan.\\ 
\hline
\end{tabular} 

\begin{tabular}{|p{0.05\textwidth}|p{0.3\textwidth}|p{0.55\textwidth}|}
\hline
20 & repartitionAndSortWithin Partitions(partitioner) & Mempartisi ulang RDD berdasarkan partitioner yang diberikan dan, dalam setiap partisi yang dihasilkan, rekord diurutkan berdasarkan key. Hal ini lebih efisien daripada memanggil repartition dan kemudian menurutkannya pada setiap partisi. \\ 
\hline
\end{tabular} 

\subsubsection{Aksi RDD}
Selain transformasi, RDD juga dapat melakukan berbagai operasi aksi. Semua aksi RDD akan mengembalikan value bukan RDD. Berikut adalah operasi-operasi aksi yang dapat dilakukan oleh RDD.\\\\
\begin{tabular}{|p{0.05\textwidth}|p{0.3\textwidth}|p{0.55\textwidth}|}
\hline 
No & Action & Arti \\ 
\hline 
1 & reduce(func) & Mengagregasi elemen dataset menggunakan fungsi func (dimana func menerima dua argumen/parameter dan mengembalikan satu). Fungsi harus komutatif dan asosiatif supaya dapat dikomputasi secara paralel dengan benar. \\ 
\hline 
2 & collect() & Mengembalikan semua elemen dari dataset sebagai sebuah array dalam driver program. Aksi ini biasanya berguna setelah melakukan filter atau operasi lain yang mengembalikan subset kecil dari data. \\ 
\hline 
3 & count() & Mengembalikan jumlah elemen di dalam dataset. \\ 
\hline 
4 & first() & Mengembalikan elemen pertama dari dataset. \\ 
\hline 
5 & take(n) & Mengembalikan sebuah array dengan n elemen pertama dari dataset \\ 
\hline 
6 & takeSample (withReplacement, num, [seed]) & Mengembalikan sebuah array dengan sampel acak dari banyak elemen dataset, dengan atau tidak replacement, menentukan random number generator seed secara optional. \\ 
\hline 
7 & takeOrdered(n, [ordering]) & Mengembalikan n elemen pertama dari RDD mengunakan antara natural ordernya atau sebuah comparator buatan sendiri. \\ 
\hline 
8 & saveAsTextFile(path) & Menulis elemen dari dataset sebagai text file di dalam directory yang diberikan (pada parameter) di dalam file sistem lokal, dalam HDFS atau file sistem Hadoop yang lain. Spark memanggil toString pada setiap elemen untuk mengubah elemen dataset menjadi baris text di dalam file. \\ 
\hline 
9 & saveAsSequenceFile(path) & Menulis elemen dari dataset sebagai Hadoop SequenceFile di dalam path yang diberikan (pada parameter) dalam file sistem lokal, HDFS atau file sistem Hadoop yang lainnya. Aksi ini tersedia dalam pasangan key-value dari RDD yang mengimplementasikan Writable Interface milik Hadoop. Di Scala, aksi ini juga tersedia dalam tipe yang secara implisit dapat diubah ke Writable (Spark menyediakan konversi untuk tipe dasar seperti Int, Double, String, dll. \\ 
\hline 
\end{tabular}

\begin{tabular}{|p{0.05\textwidth}|p{0.3\textwidth}|p{0.55\textwidth}|}
\hline 
10 & saveAsObjectFile(path) & Menulis elemen dari dataset dalam format sederhana menggunakan serialisasi Java, dimana nantinya bisa dimuat menggunakan SparkContext.objectFile().\\ 
\hline 
11 & countByKey() & Hanya tersedia pada RDD dari tipe (K,V). Mengembalikan sebuah hashmap dari pasangan (K, Int) dengan jumlah dari setiap key.\\ 
\hline 
12 & foreach(func) & Menjalankan fungsi func dalam setiap elemen di dataset. Hal ini biasanya dilakukan untuk efek samping dari suatu hal sepeti mengupdate Accumulator atau berinteraksi dengan sistem penyimpanan eksternal.\\ 
\hline 
\end{tabular} 

\subsection{Arsitektur Spark}
Dalam spark, terdapat yang namanya master node dan juga worker node. Master node bertugas untuk memanajemen pembagian RDD dan juga pembagian tugas-tugas ke semua worker node yang ada. Master node memiliki sebuah driver program. Tugas dari driver program ini adalah untuk menjalankan aplikasi yang telah dibuat. Kode yang dibuat bertindak sebagai driver program atau jika menggunakan interactive shell, shell yang bertindak sebagai driver program.

Di dalam driver program, hal pertama yang harus dilakukan adalah membuat Spark Context. Spark Context merupakan gerbang ke semua fungsi-fungsi milik spark. Semua fungsi yang akan dilakukan pada driver program akan melewati Spark Context. Driver program dan spark context bertugas menangani eksekusi pekerjaan di dalam sebuah cluster. Sebuah pekerjaan dibagi-bagi menjadi beberapa tugas-tugas kecil yang nantinya didistribusikan ke worker node. RDD juga dibuat di dalam spark context dan juga didistribusikan ke berbagai worker node. Kemudian RDD akan dicache di dalam worker node tersebut.

Worker node adalah node pekerja yang pekerjaannya adalah pada dasarnya mengerjakan tugas-tugas yang diberikan kepadanya. Tugas-tugas tersebut dieksekusi di dalam node tersebut. Tugas tersebut adalah tugas di dalam RDD yang sudah dipartisi. Kemudian, worker node  akan mengembalikan hasilnya kembali ke Spark Context.

Jadi secara garis besar, Spark Context mengambil pekerjaan, memecah pekerjaan dalam tugas-tugas/tasks dan mendistribusikannya ke worker node. Tugas-tugas tersebut bekerja pada RDD yang dipartisi, melakukan operasi, mengumpulkan hasil dan mengembalikannya ke Spark Context. 
Jika node worker ditambah, maka pekerjaan akan menjadi lebih cepat selesai. Hal ini terjadi karena pekerjaan dapat dipecah ke lebih banyak partisi dan dapat mengeksekusi pekerjaan tersebut secara paralel dalam banyak sistem yang berbeda. Besar memori juga akan bertambah yang berefek pada meningkatnya kekuatan mencache pekerjaan. Oleh karena itu, pekerjaan dapat dieksekusi dengan lebih cepat.

\section{Scala}
\label{sec:scala}
Scala atau singkatan dari Scalable Language adalah bahasa pemrograman yang berbasis Java Development Kit. Bahasa Scala menyediakan optimasi untuk kompleksitas kode dan concise notation (notasi singkat). Concise Notation artinya suatu kode memiliki banyak informasi dalam simbol atau tulisan yang sedikit yang hasilnya adalah kode yang pendek namun berbobot. Bahasa Scala juga kompatibel dengan bahasa Java. Oleh karena itu, Scala juga mendukung pemrograman berbasis objek (object-oriented) sama seperti Java. Selain itu, berkompatibel dengan Java juga memungkinkan Scala untuk menggunakan keuntungan dari Java Virtual Machine (JVM) dan juga menggunakan library dari Java. Oleh karena itu, bahasa Scala mirip dengan bahasa pemrograman Java yang digabungkan dengan bahasa pemrograman Python. Kemiripan dengan bahasa Java terletak pada inisialisasi kelas, library yang dipakai, adanya kelas main, mendukung object oriented, dan lain-lain. Sedangkan kemiripan dengan bahasa Python antara lain saat inisialisasi variabel, inisialisasi fungsi, tidak perlu semicolon(;) untuk pindah baris, dan definisi tipe data yang optional.
Untuk lebih jelasnya, bahasa pemrograman Scala digunakan karena kelebihan-kelebihannya sebagai berikut ini:

\begin{enumerate}
\item 
\item 
\item 
\end{enumerate}


\section{Teknik Klasifikasi di Spark MLlib}
\label{sec:klasifikasi}
Klasifikasi merupakan salah satu pembelajaran Machine Learning selain regresi dan clustering. Di dalam KBBI, klasifikasi adalah penyusunan bersistem dalam kelompok atau golongan menurut kaidah atau standar yang ditetapkan. Secara umum, klasifikasi adalah proses pembagian data menurut label kelas-kelas tertentu. Klasifikasi menyelesaikan permasalahan prediksi label kelas yang sifatnya diskret berbeda dengan proses regresi yang memprediksi label yang sifatnya kontinu. Teknik klasifikasi ada bermacam-macam dan dengan akurasi yang bermacam-macam pula. Dalam Spark, terdapat Spark Machine Learning Library yang sudah menyediakan pembelajaran Machine Learning secara terdistribusi. Teknik klasifikasi yang ada dalam Spark MLLib ada 5 yakni Logistic Regression, Decision Tree, Naive Bayes, Random Forest, dan juga Gradient Boosted Tree.
\begin{enumerate}
\item Logistic Regression
\item Decision Tree
\item Naive Bayes
\item Random Forest
\item Gradient Boosted Tree
\end{enumerate}
Algoritma-algoritma tersebut masih perlu dikembangkan agar dapat menangani pembuatan model klasifikasi secara inkremental. Salah satu cara untuk mengembangkan algoritma klasifikasi secara inkremental adalah dengan menggunakan Ensemble Method. Ensemble Method atau metode ensemble adalah algoritma dalam machine learning dimana algoritma ini adalah algoritma pencarian solusi prediksi terbaik dibandingkan dengan algoritma yang lain. Metode ensemble ini menggunakan beberapa algoritma pembelajaran untuk pencapaian solusi prediksi yang lebih baik daripada algoritma yang bisa diperoleh dari salah satu pembelajaran algoritma saja. Cara kerja dari metode ini adalah dengan memilih sebagian data dari data train secara acak dan membuat model dengan data train yang dipilih tersebut. Proses membuat model tersebut dilakukan berkali-kali dengan terus memilih data train secara acak. Biasanya proses tersebut dilakukan 500 sampai 1000 kali hingga terbentuk 500 hingga 1000 model klasifikasi. Kemudian, rekord yang ingin diklasifikasi dites disemua model. Proses ini menggunakan teknik bagging.

Bootstrap Aggregating atau bagging merupakan metode yang dapat memperbaiki hasil dari algoritma klasifikasi machine learning dengan menggabungkan klasifikasi prediksi dari beberapa model. Hasil klasifikasi dari beberapa model tersebut kemudian dihitung secara voting. Hasil voting kelas terbanyak akan menjadi label dari rekord yang ingin diprediksi. Tujuan dari penggabungan hasil prediksi dari beberapa model adalah untuk mengatasi ketidakstabilan pada model yang kompleks. Bagging adalah salah satu algoritma berbasis ensemble yang paling awal dan paling sederhana, namun efektif. 
 

\subsection{Source Code Algoritma Klasifikasi}
 
